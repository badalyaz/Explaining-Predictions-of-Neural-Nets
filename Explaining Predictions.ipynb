{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Convolutional Neural Network and Explaining its Predictions via Vanilla Gradient and LRP Methods (advanced)\n",
    "\n",
    "The Net is trained on the CIFAR-10 dataset and explained its predictions in terms of input variables using Gradient$\\,\\times\\,$Input, and LRP.\n",
    "\n",
    "## Part 1: Training the Convolutional Network\n",
    "\n",
    "To understand this code you need to understand the CIFAR-10 tutorial which is available at\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.default_stream(device=0) \n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the Cuda-GPU device on my PC, you can comment this step if no Cuda-GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the CIFAR-10 data is being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a convolutional neural network composed of 4 layers parameters and two pooling stages. We also initialize objects used for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(  3, 16, 3), nn.ReLU(),\n",
    "    nn.Conv2d( 16, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d( 32, 64, 5), nn.ReLU(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d( 64, 10, 5)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.95)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=20,   shuffle=True, num_workers=1)\n",
    "testloader  = torch.utils.data.DataLoader(testset,  batch_size=2000, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Net is trainig on the data. Training up to a high accuracy can take a long time, especially if the code runs on the CPU. Here, for the purpose of this demonstration, we train the network only for 3 epochs, which leads to an accuracy around 69%. If you prefer, you could also train the network for longer (e.g. 5 or 10 epochs). This should as a result significantly improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  500 accuracy: 0.424\n",
      " 1000 accuracy: 0.510\n",
      " 1500 accuracy: 0.549\n",
      " 2000 accuracy: 0.554\n",
      " 2500 accuracy: 0.600\n",
      "Epoch 2\n",
      "  500 accuracy: 0.642\n",
      " 1000 accuracy: 0.633\n",
      " 1500 accuracy: 0.660\n",
      " 2000 accuracy: 0.652\n",
      " 2500 accuracy: 0.653\n",
      "Epoch 3\n",
      "  500 accuracy: 0.671\n",
      " 1000 accuracy: 0.679\n",
      " 1500 accuracy: 0.673\n",
      " 2000 accuracy: 0.684\n",
      " 2500 accuracy: 0.692\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "\n",
    "    print('Epoch %d'%(epoch+1))\n",
    "    \n",
    "    for i,(xr,tr) in enumerate(trainloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yr = net(xr).view(-1,10)\n",
    "        criterion(yr,tr).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 500 == 499:\n",
    "\n",
    "            xt,tt = iter(testloader).next()\n",
    "            yt = net(xt).view(-1,10)\n",
    "            pt = torch.max(yt.data,dim=1)[1]\n",
    "            acc = torch.mean((pt == tt).type(torch.FloatTensor)).item()\n",
    "            print('%5d accuracy: %.3f' %(i+1,acc))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explaining the Model's Predictions\n",
    "\n",
    "Once the model has been learned, we would like to explore in more detail which input features the model uses to support the evidence it has found for the correct class. To test the different explanation methods, we will consider a small subset of 15 data points from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visloader  = torch.utils.data.DataLoader(testset,  batch_size=15, shuffle=False, num_workers=1)\n",
    "images, labels = iter(visloader).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient$\\,\\times\\,$Input\n",
    "\n",
    "Gradient$\\,\\times\\,$Input explains based on the simple gradient computation $\\mathrm{GI} = \\nabla_{x} f_c(x) \\odot x$. Here $c$ is the true class associated to the data point $x$. To visualize the explanation as a two-dimensional heatmap, obtained scores should furthermore be summed over the three color channels.\n",
    "\n",
    "We implement Gradient$\\,\\times\\,$Input and apply it to the dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ximages = images.requires_grad_(True)\n",
    "ypredimages = net(ximages).squeeze()\n",
    "criterion(ypredimages,labels)\n",
    "criterion(ypredimages,labels).backward()\n",
    "dlossdx = ximages.grad\n",
    "GI = (dlossdx*ximages).sum(dim=1)\n",
    "utils.render(images.data.numpy())\n",
    "utils.render(GI.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough capturing fine features in the data, the explanations are also relatively noisy and hard to interpret for a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-Wise Relevance Propagation\n",
    "\n",
    "Layer-Wise Relevance Propagation (LRP) is an another method for explanation that operates by backward-propagating the prediction in the network by means of local redistribution rules. A simple recursive implementation of LRP that applies the rule\n",
    "$R_i = \\sum_j \\frac{a_i w_{ij}}{\\sum_i a_i w_{ij} + b_j} R_j$\n",
    "at each layer is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp(A,layers,T,i=0):\n",
    "\n",
    "    if len(layers) == 0:\n",
    "        R = A*0\n",
    "        for i,t in enumerate(T): R[i,t] = A[i,t]\n",
    "        return R\n",
    "\n",
    "    else:\n",
    "        layer = layers[0]\n",
    "        R = lrp(layer.forward(A),layers[1:],T,i=i+1)\n",
    "        \n",
    "        A = A.clone().detach().requires_grad_(True)\n",
    "        Z = layer.forward(A)+1e-9\n",
    "        Z = layer.forward(A)+1e-9\n",
    "        (Z*(R/Z).data).sum().backward()\n",
    "        R = A*A.grad\n",
    "        return R\n",
    "    \n",
    "\n",
    "LRP = lrp(images,list(net),labels).sum(dim=1)\n",
    "\n",
    "utils.render(images.data.numpy())\n",
    "utils.render(LRP.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
